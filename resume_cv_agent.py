import fitz  # PyMuPDF
import gradio as gr
from langchain_community.llms import Ollama 
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

# ‚öôÔ∏è Mod√®le Llama3 via Ollama
llm = Ollama(model="llama3:8b")

# üìÑ Template pour structurer le r√©sum√© du CV
template = """
Voici le contenu brut d‚Äôun CV :

{cv_texte}

R√©sume ce CV de mani√®re claire pour un recruteur, en extrayant :
- Nom complet (si pr√©sent)
- Poste recherch√©
- Comp√©tences techniques
- Exp√©riences principales
- Dipl√¥mes obtenus
- Langues ma√Ætris√©es
- Outils ou technos connus

Formate tout proprement.
"""

prompt = PromptTemplate.from_template(template)
chain = LLMChain(llm=llm, prompt=prompt)

# üîç Fonction d‚Äôextraction de texte depuis un PDF
def extraire_texte_pdf(file):
    with fitz.open(file.name) as doc:
        texte = ""
        for page in doc:
            texte += page.get_text()
    return texte

# üîÅ Pipeline complet
def analyser_cv(file):
    texte = extraire_texte_pdf(file)
    if not texte.strip():
        return "Aucun texte extrait du CV. V√©rifie que le PDF n'est pas scann√© comme image."

    try:
        resume = chain.run(cv_texte=texte)
        return resume if resume else "Le mod√®le n'a pas g√©n√©r√© de r√©sum√©."
    except Exception as e:
        return f"Erreur lors de l'analyse du CV : {str(e)}"


# üé® Interface Gradio
gr.Interface(
    fn=analyser_cv,
    inputs=gr.File(label="Upload ton CV (PDF)", file_types=[".pdf"]),
    outputs=gr.Textbox(label="R√©sum√© du CV"),
    title="Agent AI - R√©sumeur de CV",
    description="Ce module lit et r√©sume un CV PDF pour un recruteur"
).launch()
